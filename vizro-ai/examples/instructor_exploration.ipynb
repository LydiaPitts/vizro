{
 "cells": [
  {
   "cell_type": "code",
   "id": "5999ceb3-7621-4121-85a2-a9766c593807",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:08:01.556944Z",
     "start_time": "2024-05-03T06:08:01.503796Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "824a0baf-6783-4640-b08a-e571ac8886cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:08:05.935538Z",
     "start_time": "2024-05-03T06:08:02.222247Z"
    }
   },
   "source": [
    "import enum\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "from typing import Literal, Optional, List, Union"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from anthropic import Anthropic",
   "id": "b6f00b4ded3c147c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:35:13.492350Z",
     "start_time": "2024-05-03T06:35:13.489019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openai\n",
    "import instructor\n",
    "\n",
    "from typing import Iterable, Literal\n",
    "from pydantic import BaseModel"
   ],
   "id": "adbf2a984ad7ba41",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "92a78d36-4884-41e1-86d7-bd42c5551cf1",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "id": "04352436-ccb7-4d06-becb-b177acdbdfac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:08:07.111688Z",
     "start_time": "2024-05-03T06:08:05.937477Z"
    }
   },
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Say this is a test\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "508dc5aa-ddf2-4601-a2c5-7c3cc434e057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "id": "b46dbaeb-deff-45fe-b77b-c41945c87d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:08:09.835914Z",
     "start_time": "2024-05-03T06:08:07.113633Z"
    }
   },
   "source": [
    "import os\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"),\n",
    "    base_url=os.environ.get(\"ANTHROPIC_API_BASE\")\n",
    ")\n",
    "\n",
    "message = client.messages.create(\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello, Claude\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ")\n",
    "print(message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text=\"Hello! It's nice to meet you. How can I assist you today?\", type='text')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "31991cc54c9968b",
   "metadata": {},
   "source": [
    "## Instructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204cb99-9f00-4c91-bcdd-0a7cb58ce1f0",
   "metadata": {},
   "source": [
    "### Minimal examples"
   ]
  },
  {
   "cell_type": "code",
   "id": "66fec53d-6f3d-49be-97f1-3e5b74b8f54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T15:58:40.156338Z",
     "start_time": "2024-05-02T15:58:40.148973Z"
    }
   },
   "source": [
    "class Labels(str, enum.Enum):\n",
    "    \"\"\"Enumeration for single-label text classification.\"\"\"\n",
    "\n",
    "    SPAM = \"spam\"\n",
    "    NOT_SPAM = \"not_spam\"\n",
    "\n",
    "\n",
    "class SinglePrediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Class for a single class label prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    class_label: Labels\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "\n",
    "def classify(data: str) -> SinglePrediction:\n",
    "    \"\"\"Perform single-label classification on the input text.\"\"\"\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        response_model=SinglePrediction,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the following text: {data}\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "# Test single-label classification\n",
    "prediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\n",
    "assert prediction.class_label == Labels.SPAM"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### README example",
   "id": "8997b1ac3ace09c4"
  },
  {
   "cell_type": "code",
   "id": "cdf1863d-0e43-4cf2-ad67-c40ab18b2f45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:10:10.459773Z",
     "start_time": "2024-05-03T06:10:09.385366Z"
    }
   },
   "source": [
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Extract structured data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
    ")\n",
    "\n",
    "print(user_info.name)\n",
    "#> John Doe\n",
    "print(user_info.age)\n",
    "#> 30"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n",
      "30\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Anthropic",
   "id": "6d742005d6d22c6c"
  },
  {
   "cell_type": "code",
   "id": "c191f4b2-5a29-49ef-a547-c67f39dbe8c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:10:18.849052Z",
     "start_time": "2024-05-03T06:10:16.879356Z"
    }
   },
   "source": [
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "client = instructor.from_anthropic(Anthropic())\n",
    "\n",
    "# note that client.chat.completions.create will also work\n",
    "resp = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=1024,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Extract Jason is 25 years old.\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=User,\n",
    ")\n",
    "\n",
    "assert isinstance(resp, User)\n",
    "assert resp.name == \"Jason\"\n",
    "assert resp.age == 25"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Felix Vemmer example",
   "id": "ef6fc44eaa10abcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ebe13-c203-4f76-989a-7381e6b89515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641bf6a-5de6-41d0-8a81-783d16f32e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "\n",
    "In the vast landscape of technological advancements, few innovations hold the promise of revolutionizing our world quite like quantum computing. It’s a realm where traditional binary systems morph into quantum bits or qubits, offering an unparalleled potential to tackle problems deemed insurmountable by classical computers. As we embark on this journey into the enigmatic depths of quantum computing, we’re met with a blend of excitement, curiosity, and a touch of trepidation. At the heart of quantum computing lies the principle of superposition, where qubits can exist in multiple states simultaneously, unlike classical bits, which are limited to either a 0 or 1. This fundamental property enables quantum computers to process vast amounts of information in parallel, promising exponential speed-ups for certain computations. Imagine solving complex optimization problems, simulating molecular structures with precision, or cracking cryptographic codes at an unprecedented pace – these are just glimpses of what quantum computing could offer. However, quantum computing isn’t merely a progression of classical computing; it’s a paradigm shift. Entanglement, another cornerstone of quantum mechanics, adds another layer of complexity. When qubits become entangled, the state of one qubit instantaneously influences the state of another, regardless of the distance between them. This phenomenon opens the door to secure communication channels and novel approaches to information processing. In conclusion, the journey into the enigmatic depths of quantum computing is filled with promise, challenges, and endless possibilities. It’s a journey that beckons us to push the boundaries of our understanding, to embrace uncertainty, and to dare to dream of a future where quantum computers empower humanity to solve the unsolvable. As we navigate this uncharted territory, one thing remains certain – the age of quantum computing has dawned, and the adventure has only just begun.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837fcb5-3e13-458c-a1c0-25bc009fae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata(BaseModel):\n",
    "    meta_title: str = Field(\n",
    "        ...,\n",
    "        min_length=50,\n",
    "        max_length=60,\n",
    "        description=\"Meta Title between 50-60 characters. Should be concise, descriptive and include primary keyword.\",\n",
    "    )\n",
    "    meta_description: str = Field(\n",
    "        ...,\n",
    "        min_length=150,\n",
    "        max_length=160,\n",
    "        description=\"Meta Description between 150-160 characters. Should be compelling, summarize the page content and include relevant keywords naturally.\",\n",
    "    )\n",
    "\n",
    "\n",
    "meta_descrioption = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_retries=5, # Retry the request 3 times\n",
    "    response_model=Metadata,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": content},\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8cc9b-ea55-49a4-95ed-99e56b1e8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another example, but with a compound typed field.\n",
    "class Actor(BaseModel):\n",
    "    name: str = Field(description=\"name of an actor\")\n",
    "    country_origin: str = Field(description=\"Country they were born in\")\n",
    "\n",
    "    @field_validator('country_origin')\n",
    "    @classmethod\n",
    "    def country_upper(cls, v: str) -> str:\n",
    "        if not v.isupper():\n",
    "            raise ValueError('Must be all caps!')\n",
    "        return v\n",
    "\n",
    "\n",
    "actor_query = \"Generate the info for a random actor that is not from the US.\"\n",
    "\n",
    "actor = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_retries=2, # Retry the request 3 times\n",
    "    response_model=Actor,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": actor_query},\n",
    "    ],\n",
    ")\n",
    "actor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Trying Page Planner functionality",
   "id": "77e7efd31666a294"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:28:21.199392Z",
     "start_time": "2024-05-03T07:28:21.196284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base = BaseModel\n",
    "query = \"I need a page with a bar and a scatter that filters on the columns 'gdpPerCap` and uses a dropdown as selector. This filter should only apply to the bar chart. The bar chart should be a stacked bar chart, while the scatter chart should be colored by the column 'continent'. I also want a table that shows the data. The title of the page should be `My wonderful jolly dashboard showing a lot of data`.\""
   ],
   "id": "481cb27ecae8b99d",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Individual models approach",
   "id": "ccc891ecb28b37da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:26:42.836843Z",
     "start_time": "2024-05-03T07:26:42.830360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AgGridPlanner(base):\n",
    "    \"\"\" Instructions relating to AgGrid or Table components.\"\"\"\n",
    "    description: str = Field(..., description=\"Description of the AgGrid or Table. Include everything that seems to relate to this component.\")\n",
    "    \n",
    "class GraphPlanner(base):\n",
    "    \"\"\"Instructions relating to Charts / Plots / Graphs. Anything we think can be created in plotly.\"\"\"\n",
    "    description: str = Field(..., description=\"Description of the Chart, Plot or Graph. Include everything that seems to relate to this component.\")\n",
    "\n",
    "class CardPlanner(base):\n",
    "    \"\"\"Instructions relating to Card /Text. Anything we think should be created in Markdown.\"\"\"\n",
    "    description: str = Field(..., description=\"Description of the Card, Text or Markdown. Include everything that seems to relate to this component.\")"
   ],
   "id": "bca41618180ef13e",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:26:43.310854Z",
     "start_time": "2024-05-03T07:26:43.305724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FilterPlanner(base):\n",
    "    description: str = Field(..., description=\"Description of the Filter. Include everything that seems to relate to this component.\")\n",
    "    column: Optional[str] = Field(None, description=\"Column to filter on.\")"
   ],
   "id": "82fd079d0d6cdb6b",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:26:58.105336Z",
     "start_time": "2024-05-03T07:26:58.098145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PagePlanner(base):\n",
    "    title: str = Field(..., description=\"Title of the page. If no description is provided, make a short and concise title from the components.\")\n",
    "    components: List[Union[AgGridPlanner,GraphPlanner,CardPlanner]] #= Field(..., description=\"List of components of the page and their dedicated description.\")\n",
    "    controls: Optional[List[FilterPlanner]]#Controls"
   ],
   "id": "f0bc78568c034459",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "response_model = PagePlanner\n",
    "\n",
    "# client = instructor.from_anthropic(Anthropic())\n",
    "# resp = client.messages.create(\n",
    "#     model=\"claude-3-opus-20240229\",\n",
    "#     max_tokens=1024,\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": query,\n",
    "#         }\n",
    "#     ],\n",
    "#     response_model=response_model,\n",
    "# )\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    response_model=response_model,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a world class extractor and your speciality is to extract dashboard components from natural language\"},{\"role\": \"user\", \"content\": query}],\n",
    ")\n",
    "resp"
   ],
   "id": "d05ab319a3f694f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### One component, but with a Literal name",
   "id": "9395dc5d85d1a45a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:35:03.350478Z",
     "start_time": "2024-05-03T07:35:03.336038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comp_type = Literal[\"AgGrid\", \"Card\", \"Graph\"]\n",
    "contr_type = Literal[\"Filter\"]\n",
    "\n",
    "class Component(BaseModel):\n",
    "#    \"\"\"Permissible options to describe components. Needs to be in the format of {\"component_name\": XXX, \"component_description\": XXX}.\"\"\"\n",
    "    component_name: comp_type\n",
    "    component_description: str = Field(..., description=\"Description of the component. Include everything that seems to relate to this component.\")\n",
    "    \n",
    "\n",
    "class Components(BaseModel):\n",
    "    components: List[Component]\n",
    "    \n",
    "class Control(BaseModel):\n",
    "#    \"\"\"Permissible options to describe controls. Needs to be in the format of {\"control_name\": XXX, \"control_description\": XXX}.\"\"\"\n",
    "    control_name: contr_type\n",
    "    control_description: str = Field(..., description=\"Description of the control. Include everything that seems to relate to this control.\")\n",
    "\n",
    "class Controls(BaseModel):\n",
    "    controls: List[Control]\n",
    "    \n",
    "class PagePlanner(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the page. If no description is provided, make a short and concise title from the components.\")\n",
    "    components: Components\n",
    "    controls: Controls#Optional[List[FilterPlanner]]#"
   ],
   "id": "c3293a4aced6cbed",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T07:35:20.623117Z",
     "start_time": "2024-05-03T07:35:03.673116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Iterable\n",
    "response_model = PagePlanner\n",
    "\n",
    "# client = instructor.from_anthropic(Anthropic())\n",
    "# resp = client.messages.create(\n",
    "#     model=\"claude-3-opus-20240229\",\n",
    "#     max_tokens=1024,\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": query,\n",
    "#         }\n",
    "#     ],\n",
    "#     response_model=response_model,\n",
    "# )\n",
    "\n",
    "client = instructor.from_openai(OpenAI())\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    response_model=response_model,\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You are a world class extractor and your speciality is to extract dashboard components from natural language\"},{\"role\": \"user\", \"content\": query}],\n",
    ")\n",
    "resp"
   ],
   "id": "9aeea7d42a021537",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PagePlanner(title='My wonderful jolly dashboard showing a lot of data', components=Components(components=[Component(component_name='Graph', component_description=\"A stacked bar chart that filters data based on the columns 'gdpPerCap'. The filter is controlled by a dropdown selector. This filter is specifically for this bar chart.\"), Component(component_name='Graph', component_description=\"A scatter chart colored by the column 'continent'.\"), Component(component_name='AgGrid', component_description='A table that shows the data.')]), controls=Controls(controls=[Control(control_name='Filter', control_description=\"This control is a dropdown that filters the stacked bar chart based on the columns 'gdpPerCap'.\")]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd901c750fa3b259"
  },
  {
   "cell_type": "markdown",
   "id": "d43649a2-2619-443a-abd2-4a1769cebe33",
   "metadata": {},
   "source": [
    "### Blog example"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><em>Thanks to the </em><em>over 11,000 people</em><em> who joined us for the first AI Engineer Su\n"
     ]
    }
   ],
   "execution_count": 32,
   "source": [
    "import feedparser\n",
    "podcast_atom_link = \"https://api.substack.com/feed/podcast/1084089.rss\" # latent space podcast\n",
    "parsed = feedparser.parse(podcast_atom_link)\n",
    "episode = [ep for ep in parsed.entries if ep['title'] == \"Why AI Agents Don't Work (yet) - with Kanjun Qiu of Imbue\"][0]\n",
    "episode_summary = episode['summary']\n",
    "print(episode_summary[:100])"
   ],
   "id": "a477bcada47ab2a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of the transcript: 60\n"
     ]
    }
   ],
   "execution_count": 33,
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "parsed_summary = partition_html(text=''.join(episode_summary)) \n",
    "start_of_transcript = [x.text for x in parsed_summary].index(\"Transcript\") + 1\n",
    "print(f\"First line of the transcript: {start_of_transcript}\")\n",
    "text = '\\n'.join(t.text for t in parsed_summary[start_of_transcript:])\n",
    "text = text[:3508] # shortening the transcript for speed & cost"
   ],
   "id": "56e55cebb042cd60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alessio: Hey everyone, welcome to the Latent Space Podcast. This is Alessio, Partner and CTO at Residence at Decibel Partners, and I'm joined by my co-host Swyx, founder of Smol.ai. [00:00:19]\\nSwyx: Hey, and today in the studio we have Kanjun from Imbue. Welcome. So you and I have, I guess, crossed paths a number of times. You're formerly named Generally Intelligent and you've just announced your rename, rebrand in huge, humongous ways. So congrats on all of that. And we're here to dive in into deeper detail on Imbue. We like to introduce you on a high level basis, but then have you go into a little bit more of your personal side. So you graduated your BS at MIT and you also spent some time at the MIT Media Lab, one of the most famous, I guess, computer hacking labs in the world. Then you graduated MIT and you went straight into BizOps at Dropbox, where you're eventually chief of staff, which is a pretty interesting role we can dive into later. And then it seems like the founder bug hit you. You were basically a three times founder at Ember, Sorceress, and now at Generally Intelligent slash Imbue. What should people know about you on the personal side that's not on your LinkedIn? That's something you're very passionate about outside of work. [00:01:12]\\nKanjun: Yeah. I think if you ask any of my friends, they would tell you that I'm obsessed with agency, like human agency and human potential. [00:01:19]\\nSwyx: That's work. Come on.\\nKanjun: It's not work. What are you talking about?\\nSwyx: So what's an example of human agency that you try to promote? [00:01:27]\\nKanjun: With all of my friends, I have a lot of conversations with them that's kind of helping figure out what's blocking them. I guess I do this with a team kind of automatically too. And I think about it for myself often, like building systems. I have a lot of systems to help myself be more effective. At Dropbox, I used to give this onboarding talk called How to Be Effective, which people liked. I think like a thousand people heard this onboarding talk, and I think maybe Dropbox was more effective. I think I just really believe that as humans, we can be a lot more than we are. And it's what drives everything. I guess completely outside of work, I do dance. I do partner dance. [00:02:03]\\nSwyx: Yeah. Lots of interest in that stuff, especially in the sort of group living houses in San Francisco, which I've been a little bit part of, and you've also run one of those. [00:02:12]\\nKanjun: That's right. Yeah. I started the archive with two friends, with Josh, my co-founder, and a couple of other folks in 2015. That's right. And GPT-3, our housemates built. [00:02:22]\\nSwyx: Was that the, I guess, the precursor to Generally Intelligent, that you started doing more things with Josh? Is that how that relationship started? Yeah. [00:02:30]\\nKanjun: This is our third company together. Our first company, Josh poached me from Dropbox for Ember. And there we built a really interesting technology, laser raster projector, VR headset. And then we were like, VR is not the thing we're most passionate about. And actually it was kind of early days when we both realized we really do believe that in our lifetimes, like computers that are intelligent are going to be able to allow us to do much more than we can do today as people and be much more as people than we can be today. And at that time, we actually, after Ember, we were like, work on AI research or start an AI lab. A bunch of our housemates were joining OpenA\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34,
   "source": "text",
   "id": "56f529c7eac73bd9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08307bf9-77bd-4cbf-8ee3-2586b605d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    school: Optional[str] = Field(..., description=\"The school this person attended\")\n",
    "    company: Optional[str] = Field(..., description=\"The company this person works for \")\n",
    "\n",
    "class People(BaseModel):\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cbd26-525f-474f-a1e9-f296875b6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e606453-be68-498a-8faf-18b6604d97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Company(BaseModel):\n",
    "    name:str\n",
    "\n",
    "class ResearchPaper(BaseModel):\n",
    "    paper_name:str = Field(..., description=\"an academic paper reference discussed\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    people: List[Person]\n",
    "    companies: List[Company]\n",
    "    research_papers: Optional[List[ResearchPaper]]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    response_model=ExtractedInfo,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8de6bf-b286-4925-9ca5-6a9e08a6600d",
   "metadata": {},
   "source": [
    "## Pydantic V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3082d16a-68e9-468a-897e-04bf3451c702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T06:34:10.752794Z",
     "start_time": "2024-05-01T06:34:10.749030Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel as BaseModelV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e5de95-199d-4451-b6b5-4c928a49d376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T06:47:52.903829Z",
     "start_time": "2024-05-01T06:47:51.986495Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "UserInfo.__init_subclass__() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m client \u001B[38;5;241m=\u001B[39m instructor\u001B[38;5;241m.\u001B[39mfrom_openai(OpenAI())\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Extract structured data from natural language\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m user_info \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt-3.5-turbo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUserInfo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mJohn Doe is 30 years old.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(user_info\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m#> John Doe\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/instructor/client.py:75\u001B[0m, in \u001B[0;36mInstructor.create\u001B[0;34m(self, response_model, messages, max_retries, validation_context, strict, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     66\u001B[0m     response_model: Type[T],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m     72\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m     73\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_kwargs(kwargs)\n\u001B[0;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_fn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstrict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/instructor/patch.py:147\u001B[0m, in \u001B[0;36mpatch.<locals>.new_create_sync\u001B[0;34m(response_model, validation_context, max_retries, strict, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnew_create_sync\u001B[39m(\n\u001B[1;32m    140\u001B[0m     response_model: Type[T_Model] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: T_ParamSpec\u001B[38;5;241m.\u001B[39mkwargs,\n\u001B[1;32m    146\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T_Model:\n\u001B[0;32m--> 147\u001B[0m     response_model, new_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mhandle_response_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m     response \u001B[38;5;241m=\u001B[39m retry_sync(\n\u001B[1;32m    151\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    152\u001B[0m         response_model\u001B[38;5;241m=\u001B[39mresponse_model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    158\u001B[0m         mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/instructor/process_response.py:216\u001B[0m, in \u001B[0;36mhandle_response_model\u001B[0;34m(response_model, mode, **kwargs)\u001B[0m\n\u001B[1;32m    214\u001B[0m     response_model \u001B[38;5;241m=\u001B[39m IterableModel(iterable_element_class)\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(response_model, OpenAISchema):\n\u001B[0;32m--> 216\u001B[0m     response_model \u001B[38;5;241m=\u001B[39m \u001B[43mopenai_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse_model\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\n\u001B[1;32m    219\u001B[0m     response_model, (IterableBase, PartialBase)\n\u001B[1;32m    220\u001B[0m ):\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream=True is not supported when using response_model parameter for non-iterables\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/instructor/function_calls.py:221\u001B[0m, in \u001B[0;36mopenai_schema\u001B[0;34m(cls)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m, (BaseModel,BaseModelV1)):\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClass must be a subclass of pydantic.BaseModel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wraps(\u001B[38;5;28mcls\u001B[39m, updated\u001B[38;5;241m=\u001B[39m())(\n\u001B[0;32m--> 221\u001B[0m     \u001B[43mcreate_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mhasattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m__name__\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43m__base__\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOpenAISchema\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m )\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/pydantic/main.py:1531\u001B[0m, in \u001B[0;36mcreate_model\u001B[0;34m(__model_name, __config__, __doc__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)\u001B[0m\n\u001B[1;32m   1528\u001B[0m     ns[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__orig_bases__\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m __base__\n\u001B[1;32m   1529\u001B[0m namespace\u001B[38;5;241m.\u001B[39mupdate(ns)\n\u001B[0;32m-> 1531\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmeta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43m__model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresolved_bases\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43m__pydantic_reset_parent_namespace__\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_create_model_module\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;18;43m__module__\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Application Support/hatch/env/virtual/vizro-ai/fZiNbsWT/vizro-ai/lib/python3.9/site-packages/pydantic/v1/main.py:282\u001B[0m, in \u001B[0;36mModelMetaclass.__new__\u001B[0;34m(mcs, name, bases, namespace, **kwargs)\u001B[0m\n\u001B[1;32m    251\u001B[0m exclude_from_namespace \u001B[38;5;241m=\u001B[39m fields \u001B[38;5;241m|\u001B[39m private_attributes\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;241m|\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__slots__\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m    252\u001B[0m new_namespace \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__config__\u001B[39m\u001B[38;5;124m'\u001B[39m: config,\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__fields__\u001B[39m\u001B[38;5;124m'\u001B[39m: fields,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    279\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{n: v \u001B[38;5;28;01mfor\u001B[39;00m n, v \u001B[38;5;129;01min\u001B[39;00m namespace\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m exclude_from_namespace},\n\u001B[1;32m    280\u001B[0m }\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__new__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmcs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_namespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# set __signature__ attr only for model class, but not for its instances\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__signature__ \u001B[38;5;241m=\u001B[39m ClassAttribute(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__signature__\u001B[39m\u001B[38;5;124m'\u001B[39m, generate_model_signature(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m, fields, config))\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/abc.py:106\u001B[0m, in \u001B[0;36mABCMeta.__new__\u001B[0;34m(mcls, name, bases, namespace, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(mcls, name, bases, namespace, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 106\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__new__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmcls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbases\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m     _abc_init(\u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: UserInfo.__init_subclass__() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "# Define your desired output structure\n",
    "class UserInfo(BaseModelV1):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "# Patch the OpenAI client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "# Extract structured data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
    ")\n",
    "\n",
    "print(user_info.name)\n",
    "#> John Doe\n",
    "print(user_info.age)\n",
    "#> 30"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Parallel function calling",
   "id": "591502c42dd80f8c"
  },
  {
   "cell_type": "code",
   "id": "59efd4d6-79b3-46fb-97bc-d21bbf30e70f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:36:37.700561Z",
     "start_time": "2024-05-03T06:36:34.259089Z"
    }
   },
   "source": [
    "class Weather(BaseModel):\n",
    "    location: str\n",
    "    units: Literal[\"imperial\", \"metric\"]\n",
    "\n",
    "\n",
    "class GoogleSearch(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "client = instructor.patch(openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)  \n",
    "\n",
    "function_calls = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You must always use tools\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n",
    "        },\n",
    "    ],\n",
    "    response_model=Iterable[Union[Weather, GoogleSearch]],  \n",
    ")\n",
    "\n",
    "for fc in function_calls:\n",
    "    print(fc)\n",
    "    #> location='Toronto' units='metric'\n",
    "    #> location='Dallas' units='imperial'\n",
    "    #> query='super bowl winner'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location='Toronto' units='metric'\n",
      "location='Dallas' units='imperial'\n",
      "query='Super Bowl winner'\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:37:48.640335Z",
     "start_time": "2024-05-03T06:37:48.634183Z"
    }
   },
   "cell_type": "code",
   "source": "fc",
   "id": "3d82f2448f6e0e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleSearch(query='Super Bowl winner')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### (Stream) Iterable - for lists of schemas",
   "id": "f29187f30e63b299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T06:41:22.136612Z",
     "start_time": "2024-05-03T06:41:20.254976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from typing import Iterable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.patch(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "users = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.1,\n",
    "    response_model=Iterable[User],\n",
    "    stream=False,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n",
    "                         Correctly segment it into entitites\\\n",
    "                        Make sure the JSON is correct\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "for user in users:\n",
    "    print(user)\n",
    "    #> name='Jason' age=10\n",
    "    #> name='John' age=30"
   ],
   "id": "96f62f685cf13ae7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Jason' age=10\n",
      "name='John' age=30\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "88bbae29-5b89-4f0a-a987-9425bf9d2444",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c2c917d4ae38e",
   "metadata": {},
   "source": [
    "Very slick library that provides a very clear interface with the underlying model\n",
    "\n",
    "PRO:\n",
    "- instantiates Pydantic models with ease\n",
    "- very simple and intuitive API\n",
    "- support for more complex technologies\n",
    "- easy support for other model vendors (not sure if this would also hold for marvin)\n",
    "\n",
    "CON:\n",
    "- only pydantic V2\n",
    "- one man show, but a good one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02563f83-c427-4648-bb93-414c945d1d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6f257-6e07-4219-879e-b2997542ea23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
